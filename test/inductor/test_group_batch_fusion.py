# Owner(s): ["module: inductor"]

import functools
import operator
import random
import unittest

import torch
import torch._inductor
from torch._dynamo.test_case import run_tests, TestCase
from torch._dynamo.utils import counters
from torch.testing._internal.inductor_utils import HAS_CUDA

try:
    # importing this will register fbgemm lowerings for inductor
    import deeplearning.fbgemm.fbgemm_gpu.fb.inductor_lowerings  # noqa: F401

    has_fbgemm = True
except Exception:
    has_fbgemm = False
    pass

requires_cuda = functools.partial(unittest.skipIf, not HAS_CUDA, "requires cuda")

torch.ops.load_library("//caffe2/torch/fb/sparsenn:sparsenn_operators")

class MyModule(torch.nn.Module):
    def __init__(self, z: int, has_bias: bool, device="cuda") -> None:
        super().__init__()
        self.z = z
        self.device = device
        self.seq_len = 10
        self.seq1 = [
            torch.nn.Linear(z, z, has_bias).to(self.device) for _ in range(self.seq_len)
        ]
        self.seq2 = [
            torch.nn.Linear(z, z, has_bias).to(self.device) for _ in range(self.seq_len)
        ]
        self.seq3 = [
            torch.nn.Linear(z, z, has_bias).to(self.device) for _ in range(self.seq_len)
        ]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x1 = [x + 0.1 * i for i in range(self.seq_len)]
        x2 = [self.seq1[i](x1[i]) for i in range(self.seq_len)]
        x3 = [x2[i] - 0.1 * i for i in range(self.seq_len)]
        x4 = [x1[i] for i in range(3)] + [x3[i] for i in range(3, self.seq_len)]
        x5 = [self.seq2[i](x4[i]) for i in range(self.seq_len)]
        x6 = [x5[i] + 0.1 * (self.seq_len - i) for i in range(self.seq_len)]
        x7 = (
            [x1[i] for i in range(4)]
            + [x3[i] for i in range(6, 8)]
            + [x6[i] for i in range(4)]
        )
        x8 = [self.seq3[i](x7[i]) for i in range(self.seq_len)]
        x9 = torch.cat(x8, dim=1)
        return x9


class MyModule2(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.linear0 = torch.nn.Linear(6, 8)
        self.linear1 = torch.nn.Linear(8, 8)
        self.linear2 = torch.nn.Linear(10, 8)
        self.linear3 = torch.nn.Linear(6, 8)
        self.linear4 = torch.nn.Linear(8, 8)
        self.linear5 = torch.nn.Linear(10, 8)
        self.bn0 = torch.nn.BatchNorm1d(8)
        self.bn1 = torch.nn.BatchNorm1d(8)
        self.bn2 = torch.nn.BatchNorm1d(8)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        t = torch.split(x, [6, 8, 10], dim=1)
        a0 = self.bn0(self.linear0(t[0] + 0.1))
        a1 = self.bn1(self.linear1(t[1] + 0.2))
        a2 = self.bn2(self.linear2(t[2] + 0.3))
        a3 = self.linear3(torch.sin(t[0]))
        a4 = self.linear4(torch.cos(t[1]))
        a5 = self.linear5(torch.sin(t[2] * 0.5))

        b = torch.cat([a0, a1, a2, a3, a4, a5])
        return torch.sigmoid(b)


class MyModule3(torch.nn.Module):
    def __init__(self, device, has_weight=True, has_bias=True):
        super().__init__()
        self.device = device
        self.scale0 = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(10)) for _ in range(5)]
        ).to(self.device)
        self.bias0 = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(10)) for _ in range(5)]
        ).to(self.device)
        self.scale1 = (
            torch.nn.ParameterList(
                [torch.nn.Parameter(torch.randn(5, 10)) for _ in range(5)]
            ).to(self.device)
            if has_weight
            else [None for _ in range(5)]
        )
        self.bias1 = (
            torch.nn.ParameterList(
                [torch.nn.Parameter(torch.randn(5, 10)) for _ in range(5)]
            ).to(self.device)
            if has_bias
            else [None for _ in range(5)]
        )

    def forward(self, x):
        l1_out = torch.split(x.to(self.device), 10, dim=2)
        post_l1 = [
            torch.nn.functional.layer_norm(
                l1_out[i], (10,), weight=self.scale0[i], bias=self.bias0[i]
            )
            for i in range(len(l1_out))
        ]
        l1_out = torch.cat(post_l1, dim=2)

        l2_out = torch.split(l1_out, 10, dim=2)
        post_l2 = [
            torch.nn.functional.layer_norm(
                l2_out[i], (5, 10), weight=self.scale1[i], bias=self.bias1[i]
            )
            for i in range(len(l2_out))
        ]

        return torch.cat(post_l2, dim=2)


class MyModule4(torch.nn.Module):
    def __init__(self, z, device, has_bias):
        super().__init__()
        self.z = z
        self.device = device
        self.has_bias = has_bias
        self.seq_len = 10
        self.weights1 = [
            torch.nn.Parameter(torch.randn(z - i % 5, z)).to(self.device)
            for i in range(self.seq_len)
        ]
        self.weights2 = [
            torch.nn.Parameter(torch.randn(z - i % 5, z)).to(self.device)
            for i in range(self.seq_len)
        ]

        if has_bias:
            self.biases1 = [
                torch.nn.Parameter(torch.randn(z - i % 5)).to(self.device)
                for i in range(self.seq_len)
            ]
            self.biases2 = [
                torch.nn.Parameter(torch.randn(z - i % 5)).to(self.device)
                for i in range(self.seq_len)
            ]

    def forward(self, x):
        x = x + 1.2
        x1 = [
            torch.nn.functional.linear(
                x, self.weights1[i], self.biases1[i] if self.has_bias else None
            )
            for i in range(self.seq_len)
        ]
        x2 = torch.cat(x1, dim=1)
        x3 = torch.split(x2, 10, dim=1)
        x4 = torch.cat(x3)
        x5 = [
            torch.nn.functional.linear(
                x4, self.weights2[i], self.biases2[i] if self.has_bias else None
            )
            for i in range(self.seq_len)
        ]
        x6 = torch.cat(x5, dim=1)
        return torch.sigmoid(x6)


class MyModule5(torch.nn.Module):
    def __init__(self, device, has_bias=True):
        super().__init__()
        self.device = device

        self.weights = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(50, 100)).to(self.device) for _ in range(5)]
        )

        self.biases = (
            ([torch.nn.Parameter(torch.randn(50)).to(self.device) for _ in range(5)])
            if has_bias
            else [None for _ in range(5)]
        )

    def forward(self, x):
        l1_out = torch.split(x.to(self.device), 100, dim=1)
        l1_linear = [
            torch.nn.functional.linear(l1_out[i], self.weights[i], self.biases[i])
            for i in range(len(l1_out))
        ]
        l1_out = torch.cat(l1_linear, dim=1)
        return torch.sin(l1_out)


class MyModule6(torch.nn.Module):
    def __init__(self, device, has_bias=True):
        super().__init__()
        self.device = device

    def forward(self, x):
        inputs = torch.split(x.to(self.device), 500, dim=1)
        x_split = torch.split(inputs[0].to(self.device), 100, dim=1)
        y_split = torch.split(inputs[1].to(self.device), 100, dim=1)
        tanh_1 = [torch.tanh(x_split[i]) for i in range(len(x_split))]
        tanh_2 = [torch.tanh(y_split[i]) for i in range(len(y_split))]
        return torch.cat(tanh_1, dim=1) + torch.cat(tanh_2, dim=1)


class MyModule7(torch.nn.Module):
    def __init__(self, device):
        super().__init__()
        self.device = device

    def forward(self, x):
        inputs = torch.unbind(x.to(self.device), dim=0)
        relu = [torch.nn.functional.relu(inputs[i]) for i in range(len(inputs))]
        return torch.stack(relu, dim=0)


class TestGroupSwishLayernorm(torch.nn.Module):
    def __init__(self, z: int, device: str) -> None:
        super().__init__()
        self.device = device
        self.linear_w0 = torch.nn.Parameter(torch.randn(z, z, device=self.device))
        self.linear_w1 = torch.nn.Parameter(torch.randn(z, z, device=self.device))
        self.linear_w2 = torch.nn.Parameter(torch.randn(z, z, device=self.device))
        self.linear_w3 = torch.nn.Parameter(torch.randn(z, z, device=self.device))
        self.linear_w4 = torch.nn.Parameter(torch.randn(z, z, device=self.device))
        self.linear_w5 = torch.nn.Parameter(torch.randn(z, z, device=self.device))

        self.linear_b0 = torch.nn.Parameter(torch.randn(z, device=self.device))
        self.linear_b1 = torch.nn.Parameter(torch.randn(z, device=self.device))
        self.linear_b2 = torch.nn.Parameter(torch.randn(z, device=self.device))
        self.linear_b3 = torch.nn.Parameter(torch.randn(z, device=self.device))
        self.linear_b4 = torch.nn.Parameter(torch.randn(z, device=self.device))
        self.linear_b5 = torch.nn.Parameter(torch.randn(z, device=self.device))

        self.ln_w0 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))
        self.ln_w1 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))
        self.ln_w2 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))
        self.ln_w3 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))
        self.ln_w4 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))
        self.ln_w5 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(1.0))

        self.ln_b0 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))
        self.ln_b1 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))
        self.ln_b2 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))
        self.ln_b3 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))
        self.ln_b4 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))
        self.ln_b5 = torch.nn.Parameter(torch.empty([z], device=self.device).fill_(0.0))

    def forward(
        self,
        t0: torch.Tensor,
        t1: torch.Tensor,
        t2: torch.Tensor,
        t3: torch.Tensor,
        t4: torch.Tensor,
        t5: torch.Tensor,
    ) -> torch.Tensor:
        t0 = t0.to(self.device)
        t1 = t1.to(self.device)
        t2 = t2.to(self.device)
        t3 = t3.to(self.device)
        t4 = t4.to(self.device)
        t5 = t5.to(self.device)
        a0 = torch.nn.functional.linear(t0, self.linear_w0, self.linear_b0)
        a1 = torch.nn.functional.linear(t1, self.linear_w1, self.linear_b1)
        a2 = torch.nn.functional.linear(t2, self.linear_w2, self.linear_b2)
        a3 = torch.nn.functional.linear(t3, self.linear_w3, self.linear_b3)
        a4 = torch.nn.functional.linear(t4, self.linear_w4, self.linear_b4)
        a5 = torch.nn.functional.linear(t5, self.linear_w5, self.linear_b5)

        b0 = torch.nn.functional.layer_norm(
            a0, a0.size()[1:], weight=self.ln_w0, bias=self.ln_b0
        )
        b1 = torch.nn.functional.layer_norm(
            a1, a1.size()[1:], weight=self.ln_w1, bias=self.ln_b1
        )
        b2 = torch.nn.functional.layer_norm(
            a2, a2.size()[1:], weight=self.ln_w2, bias=self.ln_b2
        )
        b3 = torch.nn.functional.layer_norm(
            a3, a3.size()[1:], weight=self.ln_w3, bias=self.ln_b3
        )
        b4 = torch.nn.functional.layer_norm(
            a4, a4.size()[1:], weight=self.ln_w4, bias=self.ln_b4
        )
        b5 = torch.nn.functional.layer_norm(
            a5, a5.size()[1:], weight=self.ln_w5, bias=self.ln_b5
        )

        c0 = torch.sigmoid(b0)
        c1 = torch.sigmoid(b1)
        c2 = torch.sigmoid(b2)
        c3 = torch.sigmoid(b3)
        c4 = torch.sigmoid(b4)

        c0 = torch.ops.fb.unsqueeze_n_times(c0, 0)
        c1 = torch.ops.fb.unsqueeze_n_times(c1, 0)
        c2 = torch.ops.fb.unsqueeze_n_times(c2, 0)
        c3 = torch.ops.fb.unsqueeze_n_times(c3, 0)
        c4 = torch.ops.fb.unsqueeze_n_times(c4, 0)

        mul_op = random.choice([torch.mul, operator.mul])
        d0 = mul_op(a0, c0)
        d1 = mul_op(c1, a1)
        d2 = mul_op(a2, c2)
        d3 = mul_op(a3, c3)
        d4 = mul_op(a4, c4)
        return d0 + d1 + d2 + d3 + d4 + b5


@requires_cuda()
@torch._inductor.config.patch(group_fusion=True, batch_fusion=True)
class TestGroupBatchFusion(TestCase):
    def compare_dict_tensors(self, ref_dict, res_dict, rtol=1e-3, atol=1e-3):
        if len(set(ref_dict.keys())) != len(set(res_dict.keys())):
            return False
        for key1 in ref_dict.keys():
            key2 = "_orig_mod." + key1
            assert key2 in res_dict, f"{key1} does not exist in traced module"
            if not torch.allclose(ref_dict[key1], res_dict[key2], rtol=rtol, atol=atol):
                return False
        return True

    def compare_pred(self, module, traced, input, rtol=1e-3, atol=1e-3):
        ref = module(*input)
        res = traced(*input)
        self.assertEqual(ref, res, rtol=rtol, atol=atol)

    def compare_parameters(self, module, traced, rtol=1e-3, atol=1e-3):
        ref_params = dict(module.named_parameters())
        res_params = dict(traced.named_parameters())
        self.assertTrue(self.compare_dict_tensors(ref_params, res_params, rtol, atol))

    def compare_gradients(self, module, traced, rtol=1e-3, atol=1e-3):
        ref_grad = {key: param.grad for key, param in module.named_parameters()}
        res_grad = {key: param.grad for key, param in traced.named_parameters()}
        self.assertTrue(
            self.compare_dict_tensors(ref_grad, res_grad, rtol=rtol, atol=atol)
        )

    @unittest.skipIf(not has_fbgemm, "requires fbgemm")
    def test_group_linear_fusion(self):
        z = 10
        for has_bias in [True, False]:
            counters.clear()
            module = MyModule(z, has_bias).to("cuda")
            input = [torch.randn(z, z, device="cuda")]
            traced = torch.compile(module)
            ref = module(*input)
            res = traced(*input)
            self.compare_pred(module, traced, input)
            self.assertEqual(
                counters["inductor"]["group_fusion"],
                2,
            )
            self.assertEqual(
                counters["inductor"]["batch_fusion"],
                0,
            )
            ref.sum().backward()
            res.sum().backward()
            self.compare_parameters(module, traced)
            self.compare_gradients(module, traced)
            self.assertEqual(
                counters["inductor"]["group_fusion"],
                4,
            )
            self.assertEqual(
                counters["inductor"]["batch_fusion"],
                0,
            )
            counters.clear()

    @unittest.skipIf(not has_fbgemm, "requires fbgemm")
    def test_group_linear_fusion_different_shapes(self):
        counters.clear()
        module = MyModule2().eval().to("cuda")
        input = [torch.rand(4, 24, device="cuda")]
        traced = torch.compile(module)
        ref = module(*input)
        res = traced(*input)
        self.compare_pred(module, traced, input)
        self.assertEqual(
            counters["inductor"]["group_fusion"],
            1,
        )
        self.assertEqual(
            counters["inductor"]["batch_fusion"],
            0,
        )
        ref.sum().backward()
        res.sum().backward()
        self.compare_parameters(module, traced)
        self.compare_gradients(module, traced)
        self.assertEqual(
            counters["inductor"]["group_fusion"],
            2,
        )
        self.assertEqual(
            counters["inductor"]["batch_fusion"],
            0,
        )
        counters.clear()

    def test_batch_layer_norm_fusion(self):
        for has_weight in [True, False]:
            for has_bias in [True, False]:
                counters.clear()
                module = MyModule3("cuda", has_weight, has_bias).to("cuda")
                input = [torch.randn(2, 5, 50, device="cuda")]
                traced = torch.compile(module)
                ref = module(*input)
                res = traced(*input)
                self.compare_pred(module, traced, input)
                self.assertEqual(
                    counters["inductor"]["group_fusion"],
                    0,
                )
                self.assertEqual(counters["inductor"]["batch_fusion"], 2)
                self.assertEqual(
                    counters["inductor"]["scmerge_split_removed"],
                    3,
                )
                self.assertEqual(
                    counters["inductor"]["scmerge_cat_removed"],
                    3,
                )
                ref.sum().backward()
                res.sum().backward()
                self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
                self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
                counters.clear()

    def test_batch_linear_lhs_fusion(self):
        z = 10
        for has_bias in [True, False]:
            counters.clear()
            module = MyModule4(z, "cuda", has_bias)
            input = [torch.randn(20, z, device="cuda")]
            traced = torch.compile(module)
            ref = module(*input)
            res = traced(*input)
            self.compare_pred(module, traced, input)
            self.assertEqual(counters["inductor"]["batch_fusion"], 2)
            self.assertEqual(
                counters["inductor"]["scmerge_split_removed"],
                1,
            )
            self.assertEqual(
                counters["inductor"]["scmerge_cat_removed"],
                1,
            )
            ref.sum().backward()
            res.sum().backward()
            self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
            self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
            counters.clear()

    def test_batch_linear_pre_grad_fusion(self):
        for has_bias in [True, False]:
            counters.clear()
            module = MyModule5("cuda", has_bias)
            input = [torch.randn(50, 500, device="cuda")]
            traced = torch.compile(module)
            ref = module(*input)
            res = traced(*input)
            self.compare_pred(module, traced, input)
            self.assertEqual(counters["inductor"]["batch_fusion"], 1)
            self.assertEqual(counters["inductor"]["group_fusion"], 0)
            self.assertEqual(
                counters["inductor"]["scmerge_split_removed"],
                2,
            )
            self.assertEqual(
                counters["inductor"]["scmerge_cat_removed"],
                2,
            )
            ref.sum().backward()
            res.sum().backward()
            self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
            self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
            counters.clear()

    def test_batch_tanh_pre_grad_fusion(self):
        counters.clear()
        module = MyModule6("cuda")
        input = [torch.randn(50, 1000, requires_grad=True, device="cuda")]
        traced = torch.compile(module)
        ref = module(*input)
        res = traced(*input)
        self.compare_pred(module, traced, input)
        self.assertEqual(counters["inductor"]["batch_fusion"], 2)
        self.assertEqual(
            counters["inductor"]["scmerge_split_removed"],
            2,
        )
        self.assertEqual(
            counters["inductor"]["scmerge_cat_removed"],
            2,
        )
        ref.sum().backward()
        res.sum().backward()
        self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
        self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
        counters.clear()

    def test_batch_relu_pre_grad_fusion(self):
        counters.clear()
        module = MyModule7("cuda")
        input = [torch.randn(20, 40, 60, requires_grad=True, device="cuda")]
        traced = torch.compile(module)
        ref = module(*input)
        res = traced(*input)
        self.compare_pred(module, traced, input)
        self.assertEqual(counters["inductor"]["batch_fusion"], 1)
        ref.sum().backward()
        res.sum().backward()
        self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
        self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
        counters.clear()



    @unittest.skipIf(not has_fbgemm, "requires fbgemm")
    def test_group_swish_layer_norm_pre_grad_fusion(self):
        counters.clear()
        Z = 16
        module = TestGroupSwishLayernorm(Z, "cuda")
        input = [
            torch.randn(4, Z, requires_grad=True, device="cuda"),
            torch.randn(4, Z, requires_grad=True, device="cuda"),
            torch.randn(4, Z, requires_grad=True, device="cuda"),
            torch.randn(4, Z, requires_grad=True, device="cuda"),
            torch.randn(4, Z, requires_grad=True, device="cuda"),
            torch.randn(4, Z, requires_grad=True, device="cuda"),
        ]
        traced = torch.compile(module)
        ref = module(*input)
        res = traced(*input)
        self.compare_pred(module, traced, input)
        self.assertEqual(counters["inductor"]["merge_sigmoid_unsqueeze"], 5)
        ref.sum().backward()
        res.sum().backward()
        self.compare_parameters(module, traced, rtol=1e-8, atol=1e-8)
        self.compare_gradients(module, traced, rtol=1e-8, atol=1e-8)
        counters.clear()

if __name__ == "__main__":
    run_tests()
